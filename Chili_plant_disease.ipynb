{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lH06Zlzh-jr",
        "outputId": "1bd544b1-435e-487b-d6e0-40fdaae0fab0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] =\"nafeesa21\"\n",
        "os.environ['KAGGLE_KEY'] =\"01abbacc47cae98344dd97a9361a0f46\"\n",
        "\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "api = KaggleApi()\n",
        "api.authenticate()"
      ],
      "metadata": {
        "id": "0vsHfqgDiiE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api.dataset_list_files('ahmadalmahsiri/chili-plant-disease').files\n",
        "api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCwQ3hfvh-W8",
        "outputId": "398442c8-1eff-4f8d-dac2-b1cef368ffe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<kaggle.api.kaggle_api_extended.KaggleApi at 0x7f80043178e0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api.dataset_download_files('ahmadalmahsiri/chili-plant-disease',path=\".\")"
      ],
      "metadata": {
        "id": "oks-TmHkh-J0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a7e9630-9b0d-4054-96a0-b5c4223c6a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/ahmadalmahsiri/chili-plant-disease\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Libraries"
      ],
      "metadata": {
        "id": "p0hrmJR7p9WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gva3NbhRmrQn",
        "outputId": "dfcd67fa-787c-4e70-918c-613e4ad93a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.27.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # specify GPUs locally\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from contextlib import contextmanager\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from functools import partial\n",
        "\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "import torchvision.models as models\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from albumentations import ImageOnlyTransform\n",
        "\n",
        "import timm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "CTxWYOo9foki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fecc0572-bdf7-4dd2-ad3d-852a6b46c4d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.23 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Reading the data"
      ],
      "metadata": {
        "id": "LyOg2vU8nhe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = './'\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)\n",
        "\n",
        "\n",
        "#Path to image folder\n",
        "TRAIN_PATH = '/content/chili-plant-disease.zip'"
      ],
      "metadata": {
        "id": "bsvxq2B8fow2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Configuration parameters\n"
      ],
      "metadata": {
        "id": "L8I6A7TCwOUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    debug=False\n",
        "    apex=False\n",
        "    print_freq=100\n",
        "    num_workers=4\n",
        "    model_name='resnext50_32x4d'\n",
        "    size=328\n",
        "    epochs=10\n",
        "    T_0=10 # CosineAnnealingWarmRestarts\n",
        "    lr=1e-4\n",
        "    min_lr=1e-6\n",
        "    batch_size=32\n",
        "    weight_decay=1e-6\n",
        "    gradient_accumulation_steps=1\n",
        "    max_grad_norm=1000\n",
        "    seed=42\n",
        "    target_size=5\n",
        "    target_col='label'\n",
        "    n_fold=5\n",
        "    trn_fold=[0]\n",
        "    train=True\n",
        "    smoothing=0.05\n",
        "    t1=0.3 # bi-tempered-loss temperature 1 parameter\n",
        "    t2=1.0 # bi-tempered-loss temperature 2 parameter"
      ],
      "metadata": {
        "id": "cuh1aor2fo8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utils"
      ],
      "metadata": {
        "id": "V3hF-rDewlmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions\n",
        "def get_score(y_true, y_pred):\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
        "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
        "    handler2 = FileHandler(filename=log_file)\n",
        "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    return logger\n",
        "\n",
        "LOGGER = init_logger()\n",
        "\n",
        "def seed_torch(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_torch(seed=CFG.seed)"
      ],
      "metadata": {
        "id": "_DK5QBQmwgnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data augmentations"
      ],
      "metadata": {
        "id": "zLfKkn_-xmY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transforms(*, data):\n",
        "\n",
        "    if data == 'train':\n",
        "        return A.Compose([\n",
        "            #Resize(CFG.size, CFG.size),\n",
        "            A.RandomResizedCrop(CFG.size, CFG.size),\n",
        "            A.Transpose(p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(p=0.5),\n",
        "            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
        "            A.RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
        "            A.CoarseDropout(p=0.5),\n",
        "            A.Cutout(p=0.5),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    elif data == 'valid':\n",
        "        return A.Compose([\n",
        "            A.Resize(CFG.size, CFG.size),\n",
        "            A.Normalize(\n",
        "                mean=[0.485, 0.456, 0.406],\n",
        "                std=[0.229, 0.224, 0.225],\n",
        "            ),\n",
        "            ToTensorV2(),\n",
        "        ])"
      ],
      "metadata": {
        "id": "-jrAQgdrwhDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MODEL"
      ],
      "metadata": {
        "id": "Q9LqPLi4ywG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomResNext(nn.Module):\n",
        "    def __init__(self, model_name='resnext50_32x4d', pretrained=False):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(model_name, pretrained=pretrained)\n",
        "        n_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(n_features, CFG.target_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rHY5Sqntyku-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bi-Tempered-Loss"
      ],
      "metadata": {
        "id": "Uw4roM3r0nHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_t(u, t):\n",
        "    \"\"\"Compute log_t for `u'.\"\"\"\n",
        "    if t==1.0:\n",
        "        return u.log()\n",
        "    else:\n",
        "        return (u.pow(1.0 - t) - 1.0) / (1.0 - t)\n",
        "\n",
        "def exp_t(u, t):\n",
        "    \"\"\"Compute exp_t for `u'.\"\"\"\n",
        "    if t==1:\n",
        "        return u.exp()\n",
        "    else:\n",
        "        return (1.0 + (1.0-t)*u).relu().pow(1.0 / (1.0 - t))\n",
        "\n",
        "def compute_normalization_fixed_point(activations, t, num_iters):\n",
        "\n",
        "    \"\"\"Returns the normalization value for each example (t > 1.0).\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      t: Temperature 2 (> 1.0 for tail heaviness).\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Return: A tensor of same shape as activation with the last dimension being 1.\n",
        "    \"\"\"\n",
        "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
        "    normalized_activations_step_0 = activations - mu\n",
        "\n",
        "    normalized_activations = normalized_activations_step_0\n",
        "\n",
        "    for _ in range(num_iters):\n",
        "        logt_partition = torch.sum(\n",
        "                exp_t(normalized_activations, t), -1, keepdim=True)\n",
        "        normalized_activations = normalized_activations_step_0 * \\\n",
        "                logt_partition.pow(1.0-t)\n",
        "\n",
        "    logt_partition = torch.sum(\n",
        "            exp_t(normalized_activations, t), -1, keepdim=True)\n",
        "    normalization_constants = - log_t(1.0 / logt_partition, t) + mu\n",
        "\n",
        "    return normalization_constants\n",
        "\n",
        "def compute_normalization_binary_search(activations, t, num_iters):\n",
        "\n",
        "    \"\"\"Returns the normalization value for each example (t < 1.0).\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      t: Temperature 2 (< 1.0 for finite support).\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
        "    \"\"\"\n",
        "\n",
        "    mu, _ = torch.max(activations, -1, keepdim=True)\n",
        "    normalized_activations = activations - mu\n",
        "\n",
        "    effective_dim = \\\n",
        "        torch.sum(\n",
        "                (normalized_activations > -1.0 / (1.0-t)).to(torch.int32),\n",
        "            dim=-1, keepdim=True).to(activations.dtype)\n",
        "\n",
        "    shape_partition = activations.shape[:-1] + (1,)\n",
        "    lower = torch.zeros(shape_partition, dtype=activations.dtype, device=activations.device)\n",
        "    upper = -log_t(1.0/effective_dim, t) * torch.ones_like(lower)\n",
        "\n",
        "    for _ in range(num_iters):\n",
        "        logt_partition = (upper + lower)/2.0\n",
        "        sum_probs = torch.sum(\n",
        "                exp_t(normalized_activations - logt_partition, t),\n",
        "                dim=-1, keepdim=True)\n",
        "        update = (sum_probs < 1.0).to(activations.dtype)\n",
        "        lower = torch.reshape(\n",
        "                lower * update + (1.0-update) * logt_partition,\n",
        "                shape_partition)\n",
        "        upper = torch.reshape(\n",
        "                upper * (1.0 - update) + update * logt_partition,\n",
        "                shape_partition)\n",
        "\n",
        "    logt_partition = (upper + lower)/2.0\n",
        "    return logt_partition + mu\n",
        "\n",
        "class ComputeNormalization(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Class implementing custom backward pass for compute_normalization. See compute_normalization.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, activations, t, num_iters):\n",
        "        if t < 1.0:\n",
        "            normalization_constants = compute_normalization_binary_search(activations, t, num_iters)\n",
        "        else:\n",
        "            normalization_constants = compute_normalization_fixed_point(activations, t, num_iters)\n",
        "\n",
        "        ctx.save_for_backward(activations, normalization_constants)\n",
        "        ctx.t=t\n",
        "        return normalization_constants\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        activations, normalization_constants = ctx.saved_tensors\n",
        "        t = ctx.t\n",
        "        normalized_activations = activations - normalization_constants\n",
        "        probabilities = exp_t(normalized_activations, t)\n",
        "        escorts = probabilities.pow(t)\n",
        "        escorts = escorts / escorts.sum(dim=-1, keepdim=True)\n",
        "        grad_input = escorts * grad_output\n",
        "\n",
        "        return grad_input, None, None\n",
        "\n",
        "def compute_normalization(activations, t, num_iters=5):\n",
        "    \"\"\"Returns the normalization value for each example.\n",
        "    Backward pass is implemented.\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      t: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Return: A tensor of same rank as activation with the last dimension being 1.\n",
        "    \"\"\"\n",
        "    return ComputeNormalization.apply(activations, t, num_iters)\n",
        "\n",
        "def tempered_sigmoid(activations, t, num_iters = 5):\n",
        "    \"\"\"Tempered sigmoid function.\n",
        "    Args:\n",
        "      activations: Activations for the positive class for binary classification.\n",
        "      t: Temperature tensor > 0.0.\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Returns:\n",
        "      A probabilities tensor.\n",
        "    \"\"\"\n",
        "    internal_activations = torch.stack([activations,\n",
        "        torch.zeros_like(activations)],\n",
        "        dim=-1)\n",
        "    internal_probabilities = tempered_softmax(internal_activations, t, num_iters)\n",
        "    return internal_probabilities[..., 0]\n",
        "\n",
        "\n",
        "def tempered_softmax(activations, t, num_iters=5):\n",
        "    \"\"\"Tempered softmax function.\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      t: Temperature > 1.0.\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Returns:\n",
        "      A probabilities tensor.\n",
        "    \"\"\"\n",
        "    if t == 1.0:\n",
        "        return activations.softmax(dim=-1)\n",
        "\n",
        "    normalization_constants = compute_normalization(activations, t, num_iters)\n",
        "    return exp_t(activations - normalization_constants, t)\n",
        "\n",
        "def bi_tempered_binary_logistic_loss(activations,\n",
        "        labels,\n",
        "        t1,\n",
        "        t2,\n",
        "        label_smoothing = 0.0,\n",
        "        num_iters=5,\n",
        "        reduction='mean'):\n",
        "\n",
        "    \"\"\"Bi-Tempered binary logistic loss.\n",
        "    Args:\n",
        "      activations: A tensor containing activations for class 1.\n",
        "      labels: A tensor with shape as activations, containing probabilities for class 1\n",
        "      t1: Temperature 1 (< 1.0 for boundedness).\n",
        "      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
        "      label_smoothing: Label smoothing\n",
        "      num_iters: Number of iterations to run the method.\n",
        "    Returns:\n",
        "      A loss tensor.\n",
        "    \"\"\"\n",
        "    internal_activations = torch.stack([activations,\n",
        "        torch.zeros_like(activations)],\n",
        "        dim=-1)\n",
        "    internal_labels = torch.stack([labels.to(activations.dtype),\n",
        "        1.0 - labels.to(activations.dtype)],\n",
        "        dim=-1)\n",
        "    return bi_tempered_logistic_loss(internal_activations,\n",
        "            internal_labels,\n",
        "            t1,\n",
        "            t2,\n",
        "            label_smoothing = label_smoothing,\n",
        "            num_iters = num_iters,\n",
        "            reduction = reduction)\n",
        "\n",
        "def bi_tempered_logistic_loss(activations,\n",
        "        labels,\n",
        "        t1,\n",
        "        t2,\n",
        "        label_smoothing=0.0,\n",
        "        num_iters=5,\n",
        "        reduction = 'mean'):\n",
        "\n",
        "    \"\"\"Bi-Tempered Logistic Loss.\n",
        "    Args:\n",
        "      activations: A multi-dimensional tensor with last dimension `num_classes`.\n",
        "      labels: A tensor with shape and dtype as activations (onehot),\n",
        "        or a long tensor of one dimension less than activations (pytorch standard)\n",
        "      t1: Temperature 1 (< 1.0 for boundedness).\n",
        "      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n",
        "      label_smoothing: Label smoothing parameter between [0, 1). Default 0.0.\n",
        "      num_iters: Number of iterations to run the method. Default 5.\n",
        "      reduction: ``'none'`` | ``'mean'`` | ``'sum'``. Default ``'mean'``.\n",
        "        ``'none'``: No reduction is applied, return shape is shape of\n",
        "        activations without the last dimension.\n",
        "        ``'mean'``: Loss is averaged over minibatch. Return shape (1,)\n",
        "        ``'sum'``: Loss is summed over minibatch. Return shape (1,)\n",
        "    Returns:\n",
        "      A loss tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    if len(labels.shape)<len(activations.shape): #not one-hot\n",
        "        labels_onehot = torch.zeros_like(activations)\n",
        "        labels_onehot.scatter_(1, labels[..., None], 1)\n",
        "    else:\n",
        "        labels_onehot = labels\n",
        "\n",
        "    if label_smoothing > 0:\n",
        "        num_classes = labels_onehot.shape[-1]\n",
        "        labels_onehot = ( 1 - label_smoothing * num_classes / (num_classes - 1) ) \\\n",
        "                * labels_onehot + \\\n",
        "                label_smoothing / (num_classes - 1)\n",
        "\n",
        "    probabilities = tempered_softmax(activations, t2, num_iters)\n",
        "\n",
        "    loss_values = labels_onehot * log_t(labels_onehot + 1e-10, t1) \\\n",
        "            - labels_onehot * log_t(probabilities, t1) \\\n",
        "            - labels_onehot.pow(2.0 - t1) / (2.0 - t1) \\\n",
        "            + probabilities.pow(2.0 - t1) / (2.0 - t1)\n",
        "    loss_values = loss_values.sum(dim = -1) #sum over classes\n",
        "\n",
        "    if reduction == 'none':\n",
        "        return loss_values\n",
        "    if reduction == 'sum':\n",
        "        return loss_values.sum()\n",
        "    if reduction == 'mean':\n",
        "        return loss_values.mean()"
      ],
      "metadata": {
        "id": "dU2rKWFV0UoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiTemperedLogisticLoss(nn.Module):\n",
        "    def __init__(self, t1, t2, smoothing=0.0):\n",
        "        super(BiTemperedLogisticLoss, self).__init__()\n",
        "        self.t1 = t1\n",
        "        self.t2 = t2\n",
        "        self.smoothing = smoothing\n",
        "    def forward(self, logit_label, truth_label):\n",
        "        loss_label = bi_tempered_logistic_loss(\n",
        "            logit_label, truth_label,\n",
        "            t1=self.t1, t2=self.t2,\n",
        "            label_smoothing=self.smoothing,\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "        loss_label = loss_label.mean()\n",
        "        return loss_label"
      ],
      "metadata": {
        "id": "qkPxXWdj060d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Helper functions"
      ],
      "metadata": {
        "id": "gXzfkQf90zU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    scores = AverageMeter()\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "    start = end = time.time()\n",
        "    global_step = 0\n",
        "    for step, (images, labels) in enumerate(train_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = labels.size(0)\n",
        "        y_preds = model(images)\n",
        "        loss = criterion(y_preds, labels)\n",
        "        # record loss\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        if CFG.gradient_accumulation_steps > 1:\n",
        "            loss = loss / CFG.gradient_accumulation_steps\n",
        "        else:\n",
        "            loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
        "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
        "            print('Epoch: [{0}][{1}/{2}] '\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  'Grad: {grad_norm:.4f}  '\n",
        "                  #'LR: {lr:.6f}  '\n",
        "                  .format(\n",
        "                   epoch+1, step, len(train_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses,\n",
        "                   remain=timeSince(start, float(step+1)/len(train_loader)),\n",
        "                   grad_norm=grad_norm,\n",
        "                   #lr=scheduler.get_lr()[0],\n",
        "                   ))\n",
        "    return losses.avg\n",
        "\n",
        "\n",
        "def valid_fn(valid_loader, model, criterion, device):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    scores = AverageMeter()\n",
        "    # switch to evaluation mode\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    start = end = time.time()\n",
        "    for step, (images, labels) in enumerate(valid_loader):\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = labels.size(0)\n",
        "        # compute loss\n",
        "        with torch.no_grad():\n",
        "            y_preds = model(images)\n",
        "        loss = criterion(y_preds, labels)\n",
        "        losses.update(loss.item(), batch_size)\n",
        "        # record accuracy\n",
        "\n",
        "        preds.append(y_preds.softmax(1).to('cpu').numpy())\n",
        "        if CFG.gradient_accumulation_steps > 1:\n",
        "            loss = loss / CFG.gradient_accumulation_steps\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
        "            print('EVAL: [{0}/{1}] '\n",
        "                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n",
        "                  'Elapsed {remain:s} '\n",
        "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
        "                  .format(\n",
        "                   step, len(valid_loader), batch_time=batch_time,\n",
        "                   data_time=data_time, loss=losses,\n",
        "                   remain=timeSince(start, float(step+1)/len(valid_loader)),\n",
        "                   ))\n",
        "    predictions = np.concatenate(preds)\n",
        "    return losses.avg, predictions"
      ],
      "metadata": {
        "id": "obBmko9G0U1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Train loop"
      ],
      "metadata": {
        "id": "TqSg5soM1k7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train loop\n",
        "def train_loop(folds, fold):\n",
        "\n",
        "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "\n",
        "    # loader\n",
        "    trn_idx = folds[folds['fold'] != fold].index\n",
        "    val_idx = folds[folds['fold'] == fold].index\n",
        "\n",
        "    train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
        "    valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = TrainDataset(train_folds, soft_labels_filename=soft_labels,\n",
        "                                 transform=get_transforms(data='train'))\n",
        "    valid_dataset = TrainDataset(valid_folds, soft_labels_filename=soft_labels,\n",
        "                                 transform=get_transforms(data='valid'))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              batch_size=CFG.batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset,\n",
        "                              batch_size=CFG.batch_size,\n",
        "                              shuffle=False,\n",
        "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
        "\n",
        "    #Scheduler\n",
        "    def get_scheduler(optimizer):\n",
        "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
        "        return scheduler\n",
        "\n",
        "    #Model\n",
        "    model = CustomResNext(CFG.model_name, pretrained=True)\n",
        "    model.to(device)\n",
        "\n",
        "    #Optimzer, scheduler and loss\n",
        "    optimizer = Adam(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay, amsgrad=False)\n",
        "    scheduler = get_scheduler(optimizer)\n",
        "    criterion = BiTemperedLogisticLoss(t1=CFG.t1, t2=CFG.t2, smoothing=CFG.smoothing)\n",
        "    #criterion = CrossEntropyLossOneHot\n",
        "\n",
        "\n",
        "    # loop\n",
        "    LOGGER.info(f'Criterion: {criterion}')\n",
        "\n",
        "    best_score = 0.\n",
        "    best_loss = np.inf\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        #train\n",
        "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
        "\n",
        "        #eval\n",
        "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
        "\n",
        "        #Read the labels for validation\n",
        "        if CFG.solf_label:\n",
        "            #Reverse One hot encoding\n",
        "            valid_labels=valid_folds.rename(columns={'label_0':0,'label_1':1,'label_2':2,'label_3':3,'label_4':4})\n",
        "            valid_labels['label']=valid_labels.iloc[:,1:].idxmax(axis=1)\n",
        "\n",
        "            #Compute accuracy with original labels\n",
        "            valid_labels = valid_labels[CFG.target_col].values\n",
        "        else:\n",
        "            valid_labels = valid_folds[CFG.target_col].values\n",
        "\n",
        "        #Save oof labels\n",
        "\n",
        "\n",
        "\n",
        "        if isinstance(scheduler, ReduceLROnPlateau):\n",
        "            scheduler.step(avg_val_loss)\n",
        "        elif isinstance(scheduler, CosineAnnealingLR):\n",
        "            scheduler.step()\n",
        "        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "        # scoring\n",
        "        score = get_score(valid_labels, preds.argmax(1))\n",
        "        del valid_labels\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
        "        LOGGER.info(f'Epoch {epoch+1} - Accuracy: {score}')\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
        "            torch.save({'model': model.state_dict(),\n",
        "                        'preds': preds},\n",
        "                        OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')\n",
        "\n",
        "    check_point = torch.load(OUTPUT_DIR+f'{CFG.model_name}_fold{fold}_best.pth')\n",
        "    valid_folds[[str(c) for c in range(5)]] = check_point['preds']\n",
        "    valid_folds['preds'] = check_point['preds'].argmax(1)\n",
        "\n",
        "\n",
        "    return valid_folds"
      ],
      "metadata": {
        "id": "NhoVtRqs1Nm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    if CFG.train:\n",
        "        # train\n",
        "        oof_df = pd.DataFrame()\n",
        "        for fold in range(CFG.n_fold):\n",
        "            if fold in CFG.trn_fold:\n",
        "                _oof_df = train_loop(folds, fold)\n",
        "                oof_df = pd.concat([oof_df, _oof_df])\n",
        "                LOGGER.info(f\"========== fold: {fold} result ==========\")"
      ],
      "metadata": {
        "id": "Ew-OS4PJ0U-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kqIWNhp2ixpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('chili-plant-disease.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "XUbl5JCq_hy5",
        "outputId": "a37d1c17-a0a0-42ef-96b0-ed3062c6d195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b00ae3e4-9bb8-4e1a-b07b-66c56e3d1431\", \"chili-plant-disease.zip\", 5196708)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}